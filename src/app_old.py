import streamlit as st
import sys
import os

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(__file__))

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.bedrock_service import BedrockService
from ui.streamlit_ui import (
    initialize_session_state,
    display_chat_history,
    display_performance_stats,
    display_langchain_stats,
    display_memory_stats,
    display_settings_tab,
    add_message_to_history
)

# LangChainçµ±åˆã®ç¢ºèª
try:
    from langchain_integration.bedrock_llm import create_bedrock_llm
    from langchain_integration.memory_manager import StreamlitMemoryManager
    from langchain_integration.mcp_tools import is_langchain_mcp_available
    LANGCHAIN_INTEGRATION_AVAILABLE = True
except ImportError:
    LANGCHAIN_INTEGRATION_AVAILABLE = False

# --- Streamlit UI ---

st.title("ğŸ’¬ Simple Architect Assistant")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
initialize_session_state()

# BedrockServiceã‚’åˆæœŸåŒ–ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ç®¡ç†ï¼‰
if "bedrock_service" not in st.session_state:
    st.session_state.bedrock_service = BedrockService()

bedrock_service = st.session_state.bedrock_service


# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‘ã‚¹
SYSTEM_PROMPT_FILE = os.path.join(os.path.dirname(
    __file__), "prompts", "solution_architect_system_prompt.txt")
SYSTEM_PROMPT = load_system_prompt(SYSTEM_PROMPT_FILE)

# LangChain Bedrock LLMåˆæœŸåŒ–
langchain_llm = None
memory_manager = None
if LANGCHAIN_INTEGRATION_AVAILABLE:
    langchain_llm = create_bedrock_llm(aws_profile, aws_region, MODEL_ID)
    memory_manager = get_memory_manager(window_size=10)

# --- Bedrockå‘¼ã³å‡ºã—é–¢æ•° (ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ) ---


def invoke_bedrock_streaming(prompt: str, enable_cache: bool = True, use_langchain: bool = True):
    """Bedrockã®Claude 4 Sonnetãƒ¢ãƒ‡ãƒ«ã‚’Converse APIã¾ãŸã¯LangChainã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—"""
    
    # LangChainã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    if use_langchain and LANGCHAIN_INTEGRATION_AVAILABLE and langchain_llm:
        return invoke_with_langchain(prompt)
    
    # å¾“æ¥ã®Converse APIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    return invoke_with_converse_api(prompt, enable_cache)

def invoke_with_langchain(prompt: str):
    """LangChainã‚’ä½¿ç”¨ã—ãŸBedrockå‘¼ã³å‡ºã—ï¼ˆãƒ¡ãƒ¢ãƒªç®¡ç†ä»˜ãï¼‰"""
    try:
        # çµ±è¨ˆæ›´æ–°
        st.session_state["cache_stats"]["total_requests"] += 1
        
        # ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å–å¾—
        if memory_manager and memory_manager.is_available():
            chat_history = memory_manager.get_chat_history()[:-1]  # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é™¤ã
        else:
            chat_history = st.session_state.get("messages", [])[:-1]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # LangChainã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
        for chunk in langchain_llm.invoke_with_memory(prompt, SYSTEM_PROMPT, chat_history):
            yield chunk
            
        # LangChainä½¿ç”¨çµ±è¨ˆã‚’æ›´æ–°
        if "langchain_stats" not in st.session_state:
            st.session_state["langchain_stats"] = {
                "total_requests": 0,
                "successful_requests": 0
            }
        
        st.session_state["langchain_stats"]["total_requests"] += 1
        st.session_state["langchain_stats"]["successful_requests"] += 1
        
    except Exception as e:
        st.error(f"LangChain Bedrockã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        yield "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€LangChainã§ã®å¿œç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

def invoke_with_converse_api(prompt: str, enable_cache: bool = True):
    """å¾“æ¥ã®Converse APIã‚’ä½¿ç”¨ã—ãŸBedrockå‘¼ã³å‡ºã—"""
    try:
        # çµ±è¨ˆæ›´æ–°
        st.session_state["cache_stats"]["total_requests"] += 1
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«å½¢å¼ï¼‰
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "text": f"{SYSTEM_PROMPT}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•: {prompt}"
                    }
                ]
            }
        ]

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ†é›¢
        if enable_cache:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
            system_prompts = [
                {
                    "text": SYSTEM_PROMPT
                },
                {
                    "cachePoint": {"type": "default"}
                }
            ]
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "text": prompt
                        }
                    ]
                }
            ]
            
            # Converse APIã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãï¼‰
            response = bedrock_client.converse_stream(
                modelId=MODEL_ID,
                messages=messages,
                system=system_prompts,
                inferenceConfig={
                    "maxTokens": 4096,
                    "temperature": 0.7
                }
            )
        else:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®å ´åˆ
            response = bedrock_client.converse_stream(
                modelId=MODEL_ID,
                messages=messages,
                inferenceConfig={
                    "maxTokens": 4096,
                    "temperature": 0.7
                }
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ¨å®šï¼ˆ2å›ç›®ä»¥é™ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ™‚ï¼‰
        if enable_cache and st.session_state["cache_stats"]["total_requests"] > 1:
            st.session_state["cache_stats"]["cache_hits"] += 1
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®šï¼ˆç´„600ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
            st.session_state["cache_stats"]["total_tokens_saved"] += 600

        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿ã€é€æ¬¡yieldã™ã‚‹
        if response and "stream" in response:
            for event in response["stream"]:
                if "contentBlockDelta" in event:
                    delta = event["contentBlockDelta"]["delta"]
                    if "text" in delta:
                        yield delta["text"]
                elif "messageStop" in event:
                    break
        else:
            st.error("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            yield "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    except Exception as e:
        st.error(f"Converse APIã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        yield "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

# --- Streamlit UI ---


st.title("ğŸ’¬ Simple Architect Assistant")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’è¡¨ç¤º
with st.sidebar:
    st.header("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "cache_stats" not in st.session_state:
        st.session_state["cache_stats"] = {
            "total_requests": 0,
            "cache_hits": 0,
            "total_tokens_saved": 0
        }
    
    stats = st.session_state["cache_stats"]
    
    # çµ±è¨ˆè¡¨ç¤º
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°", stats["total_requests"])
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ•°", stats["cache_hits"])
    
    with col2:
        cache_hit_rate = (stats["cache_hits"] / max(stats["total_requests"], 1)) * 100
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡", f"{cache_hit_rate:.1f}%")
        st.metric("ç¯€ç´„ãƒˆãƒ¼ã‚¯ãƒ³æ•°(æ¨å®š)", stats["total_tokens_saved"])
    
    # LangChainçµ±è¨ˆè¡¨ç¤º
    if LANGCHAIN_INTEGRATION_AVAILABLE and langchain_llm:
        st.header("ğŸ¦œ LangChainçµ±è¨ˆ")
        
        if "langchain_stats" not in st.session_state:
            st.session_state["langchain_stats"] = {
                "total_requests": 0,
                "successful_requests": 0
            }
        
        lc_stats = st.session_state["langchain_stats"]
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("LangChainç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ", lc_stats["total_requests"])
        with col2:
            success_rate = (lc_stats["successful_requests"] / max(lc_stats["total_requests"], 1)) * 100
            st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")
        
        # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆè¡¨ç¤º
        if memory_manager and memory_manager.is_available():
            st.subheader("ğŸ’­ ä¼šè©±ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ")
            
            conversation_analysis = ConversationAnalyzer.analyze_conversation(
                memory_manager.get_chat_history()
            )
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ä¼šè©±ã‚¿ãƒ¼ãƒ³æ•°", conversation_analysis.get("total_messages", 0))
                st.metric("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", conversation_analysis.get("user_message_count", 0))
            with col2:
                st.metric("AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", conversation_analysis.get("ai_message_count", 0))
                if conversation_analysis.get("topics"):
                    st.write("**æ¤œå‡ºãƒˆãƒ”ãƒƒã‚¯:**", ", ".join(conversation_analysis["topics"][:3]))
                    
            # ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒœã‚¿ãƒ³
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
                    if memory_manager:
                        memory_manager.clear_history()
                    st.session_state["messages"] = [
                        {"role": "assistant", "content": "ã©ã®ã‚ˆã†ãªAWSæ§‹æˆã«é–¢å¿ƒãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}
                    ]
                    st.rerun()
            with col2:
                if st.button("ä¼šè©±å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
                    if memory_manager:
                        history = memory_manager.export_history()
                        st.download_button(
                            "å±¥æ­´ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=json.dumps(history, ensure_ascii=False, indent=2),
                            file_name="chat_history.json",
                            mime="application/json"
                        )

    # è¨­å®š
    st.header("âš™ï¸ è¨­å®š")
    enable_cache = st.checkbox("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=True)
    
    if LANGCHAIN_INTEGRATION_AVAILABLE and langchain_llm:
        use_langchain = st.checkbox("LangChainã‚’ä½¿ç”¨ã™ã‚‹", value=True)
        
        with st.expander("LangChainè©³ç´°è¨­å®š"):
            st.write("**åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:**")
            st.write("âœ… AWS Bedrockçµ±åˆ")
            st.write("âœ… ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç®¡ç†")
            st.write("âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
            if is_langchain_mcp_available():
                st.write("âœ… MCP Toolsçµ±åˆ")
            else:
                st.write("âŒ MCP Toolsçµ±åˆï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
    else:
        use_langchain = False
        st.info("ğŸ’¡ LangChainçµ±åˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    
    if st.button("çµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state["cache_stats"] = {
            "total_requests": 0,
            "cache_hits": 0,
            "total_tokens_saved": 0
        }
        if "langchain_stats" in st.session_state:
            st.session_state["langchain_stats"] = {
                "total_requests": 0,
                "successful_requests": 0
            }
        st.rerun()

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç®¡ç†
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ã©ã®ã‚ˆã†ãªAWSæ§‹æˆã«é–¢å¿ƒãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}]

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›
if prompt := st.chat_input():
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # LangChainãƒ¡ãƒ¢ãƒªç®¡ç†ã«ã‚‚è¿½åŠ 
    if memory_manager and memory_manager.is_available():
        memory_manager.add_user_message(prompt)
    
    st.chat_message("user").write(prompt)

    # Bedrockã‹ã‚‰ã®å¿œç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å–å¾—ã—è¡¨ç¤º
    with st.chat_message("assistant"):
        with st.spinner("AIãŒå¿œç­”ã‚’ç”Ÿæˆä¸­ã§ã™..."):
            full_response = ""
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆã—ã€é€æ¬¡æ›´æ–°ã™ã‚‹
            message_placeholder = st.empty()
            for chunk in invoke_bedrock_streaming(prompt, enable_cache, use_langchain if 'use_langchain' in locals() else False):
                full_response += chunk
                message_placeholder.write(full_response + "â–Œ")  # ã‚«ãƒ¼ã‚½ãƒ«è¡¨ç¤º
            message_placeholder.write(full_response)  # æœ€çµ‚çš„ãªå¿œç­”ã‚’è¡¨ç¤º

    # AIã®å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append(
        {"role": "assistant", "content": full_response})
    
    # LangChainãƒ¡ãƒ¢ãƒªç®¡ç†ã«ã‚‚è¿½åŠ 
    if memory_manager and memory_manager.is_available():
        memory_manager.add_ai_message(full_response)
