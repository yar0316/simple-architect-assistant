"""Streamlit UIé–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
import streamlit as st
import json
from typing import Dict, Any, Optional

def initialize_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–"""
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ã©ã®ã‚ˆã†ãªAWSæ§‹æˆã«é–¢å¿ƒãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}
        ]
    
    if "cache_stats" not in st.session_state:
        st.session_state["cache_stats"] = {
            "total_requests": 0,
            "cache_hits": 0,
            "total_tokens_saved": 0
        }
    
    if "langchain_stats" not in st.session_state:
        st.session_state["langchain_stats"] = {
            "total_requests": 0,
            "successful_requests": 0
        }

def display_chat_history(messages=None):
    """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º"""
    if messages is None:
        messages = st.session_state.messages
    for msg in messages:
        st.chat_message(msg["role"]).write(msg["content"])

def add_message_to_history(role: str, content: str):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ """
    st.session_state.messages.append({"role": role, "content": content})

def display_performance_stats():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’è¡¨ç¤º"""
    st.header("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ")
    stats = st.session_state["cache_stats"]
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°", stats["total_requests"])
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ•°", stats["cache_hits"])
    
    with col2:
        cache_hit_rate = (stats["cache_hits"] / max(stats["total_requests"], 1)) * 100
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡", f"{cache_hit_rate:.1f}%")
        st.metric("ç¯€ç´„ãƒˆãƒ¼ã‚¯ãƒ³æ•°(æ¨å®š)", stats["total_tokens_saved"])

def display_langchain_stats():
    """LangChainçµ±è¨ˆã‚’è¡¨ç¤º"""
    if "langchain_stats" in st.session_state:
        st.header("ğŸ¦œ LangChainçµ±è¨ˆ")
        lc_stats = st.session_state["langchain_stats"]
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("LangChainç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ", lc_stats["total_requests"])
        with col2:
            success_rate = (lc_stats["successful_requests"] / max(lc_stats["total_requests"], 1)) * 100
            st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")

def display_memory_stats(memory_manager):
    """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’è¡¨ç¤º"""
    if memory_manager and memory_manager.is_available():
        try:
            from langchain_integration.memory_manager import ConversationAnalyzer
            
            st.subheader("ğŸ’­ ä¼šè©±ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ")
            conversation_analysis = ConversationAnalyzer.analyze_conversation(
                memory_manager.get_chat_history()
            )
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ä¼šè©±ã‚¿ãƒ¼ãƒ³æ•°", conversation_analysis.get("total_messages", 0))
                st.metric("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", conversation_analysis.get("user_message_count", 0))
            with col2:
                st.metric("AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", conversation_analysis.get("ai_message_count", 0))
                if conversation_analysis.get("topics"):
                    st.write("**æ¤œå‡ºãƒˆãƒ”ãƒƒã‚¯:**", ", ".join(conversation_analysis["topics"][:3]))
            
            # ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒœã‚¿ãƒ³
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
                    memory_manager.clear_history()
                    st.session_state.messages = [
                        {"role": "assistant", "content": "ã©ã®ã‚ˆã†ãªAWSæ§‹æˆã«é–¢å¿ƒãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}
                    ]
                    st.rerun()
            with col2:
                if st.button("ä¼šè©±å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
                    history = memory_manager.export_history()
                    st.download_button(
                        "å±¥æ­´ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=json.dumps(history, ensure_ascii=False, indent=2),
                        file_name="chat_history.json",
                        mime="application/json"
                    )
        except ImportError:
            st.info("ä¼šè©±åˆ†ææ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

def display_settings_tab(bedrock_service):
    """è¨­å®šã‚¿ãƒ–ã‚’è¡¨ç¤º"""
    st.header("âš™ï¸ è¨­å®š")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    enable_cache = st.checkbox(
        "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
        value=st.session_state.get("enable_cache", True)
    )
    st.session_state["enable_cache"] = enable_cache
    
    # LangChainè¨­å®š
    if bedrock_service.is_langchain_available():
        use_langchain = st.checkbox(
            "LangChainã‚’ä½¿ç”¨ã™ã‚‹",
            value=st.session_state.get("use_langchain", True)
        )
        st.session_state["use_langchain"] = use_langchain
        
        with st.expander("LangChainè©³ç´°è¨­å®š"):
            st.write("**åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:**")
            st.write("âœ… AWS Bedrockçµ±åˆ")
            st.write("âœ… ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç®¡ç†")
            st.write("âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
            
            try:
                from langchain_integration.mcp_tools import is_langchain_mcp_available
                if is_langchain_mcp_available():
                    st.write("âœ… MCP Toolsçµ±åˆ")
                else:
                    st.write("âŒ MCP Toolsçµ±åˆï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
            except ImportError:
                st.write("âŒ MCP Toolsçµ±åˆï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
    else:
        st.session_state["use_langchain"] = False
        st.info("ğŸ’¡ LangChainçµ±åˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    
    # çµ±è¨ˆãƒªã‚»ãƒƒãƒˆ
    if st.button("çµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state["cache_stats"] = {
            "total_requests": 0,
            "cache_hits": 0,
            "total_tokens_saved": 0
        }
        st.session_state["langchain_stats"] = {
            "total_requests": 0,
            "successful_requests": 0
        }
        st.rerun()

def handle_chat_input(bedrock_service, memory_manager=None):
    """ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’å‡¦ç†"""
    if prompt := st.chat_input():
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        add_message_to_history("user", prompt)
        
        # ãƒ¡ãƒ¢ãƒªç®¡ç†ã«ã‚‚è¿½åŠ 
        if memory_manager and memory_manager.is_available():
            memory_manager.add_user_message(prompt)
        
        st.chat_message("user").write(prompt)
        
        # AIå¿œç­”ã‚’å–å¾—
        with st.chat_message("assistant"):
            with st.spinner("AIãŒå¿œç­”ã‚’ç”Ÿæˆä¸­ã§ã™..."):
                full_response = ""
                message_placeholder = st.empty()
                
                # è¨­å®šå€¤ã‚’å–å¾—
                enable_cache = st.session_state.get("enable_cache", True)
                use_langchain = st.session_state.get("use_langchain", True)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‡¦ç†
                for chunk in bedrock_service.invoke_streaming(
                    prompt=prompt,
                    enable_cache=enable_cache,
                    use_langchain=use_langchain
                ):
                    full_response += chunk
                    message_placeholder.write(full_response + "â–Œ")
                
                message_placeholder.write(full_response)
        
        # AIå¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
        add_message_to_history("assistant", full_response)
        
        # ãƒ¡ãƒ¢ãƒªç®¡ç†ã«ã‚‚è¿½åŠ 
        if memory_manager and memory_manager.is_available():
            memory_manager.add_ai_message(full_response)

# ã‚³ã‚¹ãƒˆåˆ†æé–¢é€£ã®æ©Ÿèƒ½ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã®cost_analysisãƒ„ãƒ¼ãƒ«ãŒè‡ªå‹•çš„ã«ã‚³ã‚¹ãƒˆè¨ˆç®—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚