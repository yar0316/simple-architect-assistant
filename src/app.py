import streamlit as st
import boto3
import json
import os

# --- AWS Bedrockã®è¨­å®š ---

# Streamlitã®Secretsã‹ã‚‰AWSè¨­å®šã‚’èª­ã¿è¾¼ã‚€
try:
    aws_profile = st.secrets.aws.profile
    aws_region = st.secrets.aws.region
except (AttributeError, KeyError):
    st.error("AWSã®è¨­å®šãŒ.streamlit/secrets.tomlã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

# boto3ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¨Bedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
session = boto3.Session(profile_name=aws_profile)
bedrock_client = session.client("bedrock-runtime", region_name=aws_region)

# ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ID (Claude 4 Sonnet)
# Inference Profileã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«IDã®å…ˆé ­ã« 'us.' ã‚’ä»˜ä¸ã™ã‚‹
MODEL_ID = "us.anthropic.claude-sonnet-4-20250514-v1:0"

# --- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª­ã¿è¾¼ã¿ ---


def load_system_prompt(file_path: str) -> str:
    """æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€é–¢æ•°"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        st.error(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        st.stop()


# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‘ã‚¹
SYSTEM_PROMPT_FILE = os.path.join(os.path.dirname(
    __file__), "prompts", "solution_architect_system_prompt.txt")
SYSTEM_PROMPT = load_system_prompt(SYSTEM_PROMPT_FILE)

# --- Bedrockå‘¼ã³å‡ºã—é–¢æ•° (ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ) ---


def invoke_bedrock_streaming(prompt: str, enable_cache: bool = True):
    """Bedrockã®Claude 4 Sonnetãƒ¢ãƒ‡ãƒ«ã‚’Converse APIã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—"""
    try:
        # çµ±è¨ˆæ›´æ–°
        st.session_state["cache_stats"]["total_requests"] += 1
        
        if enable_cache:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": SYSTEM_PROMPT
                        },
                        {
                            "type": "cacheControl",
                            "cacheType": "ephemeral"
                        },
                        {
                            "type": "text", 
                            "text": f"\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•: {prompt}"
                        }
                    ]
                }
            ]
        else:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"{SYSTEM_PROMPT}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•: {prompt}"
                        }
                    ]
                }
            ]

        # Converse APIã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—
        response = bedrock_client.converse_stream(
            modelId=MODEL_ID,
            messages=messages,
            inferenceConfig={
                "maxTokens": 4096,
                "temperature": 0.7
            }
        )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ¨å®šï¼ˆ2å›ç›®ä»¥é™ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ™‚ï¼‰
        if enable_cache and st.session_state["cache_stats"]["total_requests"] > 1:
            st.session_state["cache_stats"]["cache_hits"] += 1
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®šï¼ˆç´„600ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
            st.session_state["cache_stats"]["total_tokens_saved"] += 600

        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿ã€é€æ¬¡yieldã™ã‚‹  
        for event in response.get("stream", []):
            if "contentBlockDelta" in event:
                delta = event["contentBlockDelta"]["delta"]
                if "text" in delta:
                    yield delta["text"]
            elif "messageStop" in event:
                break

    except Exception as e:
        st.error(f"Bedrockã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        yield "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

# --- Streamlit UI ---


st.title("ğŸ’¬ Simple Architect Assistant")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’è¡¨ç¤º
with st.sidebar:
    st.header("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "cache_stats" not in st.session_state:
        st.session_state["cache_stats"] = {
            "total_requests": 0,
            "cache_hits": 0,
            "total_tokens_saved": 0
        }
    
    stats = st.session_state["cache_stats"]
    
    # çµ±è¨ˆè¡¨ç¤º
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°", stats["total_requests"])
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ•°", stats["cache_hits"])
    
    with col2:
        cache_hit_rate = (stats["cache_hits"] / max(stats["total_requests"], 1)) * 100
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡", f"{cache_hit_rate:.1f}%")
        st.metric("ç¯€ç´„ãƒˆãƒ¼ã‚¯ãƒ³æ•°(æ¨å®š)", stats["total_tokens_saved"])
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    st.header("âš™ï¸ è¨­å®š")
    enable_cache = st.checkbox("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=True)
    
    if st.button("çµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state["cache_stats"] = {
            "total_requests": 0,
            "cache_hits": 0,
            "total_tokens_saved": 0
        }
        st.rerun()

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç®¡ç†
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ã©ã®ã‚ˆã†ãªAWSæ§‹æˆã«é–¢å¿ƒãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}]

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›
if prompt := st.chat_input():
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¦è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # Bedrockã‹ã‚‰ã®å¿œç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å–å¾—ã—è¡¨ç¤º
    with st.chat_message("assistant"):
        with st.spinner("AIãŒå¿œç­”ã‚’ç”Ÿæˆä¸­ã§ã™..."):
            full_response = ""
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆã—ã€é€æ¬¡æ›´æ–°ã™ã‚‹
            message_placeholder = st.empty()
            for chunk in invoke_bedrock_streaming(prompt, enable_cache):
                full_response += chunk
                message_placeholder.write(full_response + "â–Œ")  # ã‚«ãƒ¼ã‚½ãƒ«è¡¨ç¤º
            message_placeholder.write(full_response)  # æœ€çµ‚çš„ãªå¿œç­”ã‚’è¡¨ç¤º

    # AIã®å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append(
        {"role": "assistant", "content": full_response})
